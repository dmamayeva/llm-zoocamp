{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "993d97bb-3a27-4fa7-afcb-0626afd73613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23c60d60-09d8-44b2-8848-af2a9a85181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import minsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87a95a97-5b76-4749-9ea7-4ea8ed01a42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f411adb7-2e55-4283-a340-b557d7fd013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('documents.json', 'rt') as f_in:\n",
    "    docs_raw = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d691c5b-1cbe-4da1-a9a2-8a4e3bd721b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for course_dict in docs_raw:\n",
    "    for doc in course_dict['documents']:\n",
    "        doc['course'] = course_dict['course']\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fd208fa-3471-4f4f-bbee-bb0c5ce69798",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = minsearch.Index(\n",
    "    text_fields=['question', 'text', 'section'],\n",
    "                keyword_fields=['course']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8090b27c-b6b0-4d53-8bbf-865cb4cd9d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x7e0584c40730>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09f3eafc-034c-4592-ba5b-f40c1dfefbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c39c84bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b12e2604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course':'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4afa338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database. \n",
    "Use only the facts from the CONTEXT when answering the question.\n",
    "\n",
    "QUESTION : {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\".strip()\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daed493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=[{\"role\":'user', 'content':prompt}]\n",
    ")\n",
    "    response.choices[0].message.content\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36febed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'how do I run kafka?'\n",
    "def rag(query):\n",
    "    results = search(query)\n",
    "    prompt = build_prompt(query, results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84d1ef88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, you can still enroll in the course even after it has started. You are eligible to submit the homework, but be mindful of the deadlines for turning in the final projects. It's advised not to leave everything to the last minute.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag('the course has already started, can I still enroll?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afe24c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "32a0c965",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_client = Elasticsearch('http://localhost:9200')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "400f8401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': '9fbe22548c63', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'wU2FPpDNTjmBOs2UEeI3Vw', 'version': {'number': '8.4.3', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '42f05b9372a9a4a470db3b52817899b99a76ee73', 'build_date': '2022-10-04T07:17:24.662462378Z', 'build_snapshot': False, 'lucene_version': '9.3.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_client.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a6d199ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'course_questions'})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"} \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"course_questions\"\n",
    "\n",
    "es_client.indices.create(index=index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "943dc8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "06494bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 948/948 [00:28<00:00, 33.71it/s]\n"
     ]
    }
   ],
   "source": [
    "for doc in tqdm(documents):\n",
    "    es_client.index(index=index_name, document=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b5d38298",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'I just discovered the course. Can I still join?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0b856369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search(query):\n",
    "    search_query = {\n",
    "    \"size\": 5,\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": {\n",
    "                \"multi_match\": {\n",
    "                    \"query\": query,\n",
    "                    \"fields\": [\"question^3\", \"text\", \"section\"],\n",
    "                    \"type\": \"best_fields\"\n",
    "                }\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"term\": {\n",
    "                    \"course\": \"data-engineering-zoomcamp\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    }\n",
    "    response = es_client.search(index=index_name, body=search_query)\n",
    "    result_docs = []\n",
    "    for hit in response['hits']['hits']:\n",
    "        result_docs.append(hit['_source'])\n",
    "    \n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c03a6606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"Answer: To run the provided code, ensure that the 'dlt[duckdb]' package is installed. You can do this by executing the provided installation command: !pip install dlt[duckdb]. If you’re doing it locally, be sure to also have duckdb pip installed (even before the duckdb package is loaded).\",\n",
       "  'section': 'Workshop 1 - dlthub',\n",
       "  'question': 'How do I install the necessary dependencies to run the code?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'In the project directory, run:\\njava -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Java Kafka: How to run producer/consumer/kstreams/etc in terminal',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\\nHaving this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\\nYou will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\\nRemember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\\nThis is also a great resource: https://dangitgit.com/',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'How do I use Git / GitHub for this course?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': \"Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\\nTo create a virtual env and install packages (run only once)\\npython -m venv env\\nsource env/bin/activate\\npip install -r ../requirements.txt\\nTo activate it (you'll need to run it every time you need the virtual env):\\nsource env/bin/activate\\nTo deactivate it:\\ndeactivate\\nThis works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\\nAlso the virtual environment should be created only to run the python file. Docker images should first all be up and running.\",\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Module “kafka” not found when trying to run producer.py',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'You can check the version of your local spark using spark-submit --version. In the build.sh file of the Python folder, make sure that SPARK_VERSION matches your local version. Similarly, make sure the pyspark you pip installed also matches this version.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'How do I check compatibility of local and container Spark versions?',\n",
       "  'course': 'data-engineering-zoomcamp'}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elastic_search('How do I run kafka?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cef04cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'how do I run kafka?'\n",
    "def rag(query):\n",
    "    results = elastic_search(query)\n",
    "    prompt = build_prompt(query, results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ccc284d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To run Kafka, based on the provided context from Module 6: streaming with kafka, you can follow the instructions specific to running Java Kafka components like producer/consumer/kstreams in the terminal. Here’s the relevant information extracted from the context:\\n\\n1. For running a Java Kafka component like a producer in the terminal, navigate to your project directory.\\n2. Execute the following command:\\n   ```plaintext\\n   java -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java\\n   ```\\n\\nEnsure you replace `<jar_name>` with the actual name of your jar file.\\n\\nIf you are dealing with a Python Kafka producer, make sure to:\\n1. Create a virtual environment:\\n   ```plaintext\\n   python -m venv env\\n   source env/bin/activate  # On Windows use env\\\\Scripts\\\\activate\\n   ```\\n2. Install the necessary dependencies:\\n   ```plaintext\\n   pip install -r ../requirements.txt\\n   ```\\n\\n3. Activate the virtual environment whenever you need to run your Kafka Python files:\\n   ```plaintext\\n   source env/bin/activate  # On Windows use env\\\\Scripts\\\\activate\\n   ```\\nTo deactivate the virtual environment, simply run:\\n   ```plaintext\\n   deactivate\\n   ```\\n\\nNote: The virtual environment should be created only to run the Python file, and ensure Docker images are up and running if you are using Docker.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
